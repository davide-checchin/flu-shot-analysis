{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "147189f4-f887-445e-b94f-e7325e374436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b146eda1-3f2f-428a-90dc-e83ef1a0c451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26707, 36)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"../data/training_set_features.csv\")\n",
    "y_train = pd.read_csv(\"../data/training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"../data/test_set_features.csv\")\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bfd519-652e-48e0-af6d-c3e2fad7de90",
   "metadata": {},
   "source": [
    "## First looks to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6fafdc-0599-46c2-9b41-9f3e7d28bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.head(3))\n",
    "print(X_train.describe())\n",
    "print(y_train.head(3))\n",
    "print(y_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12439c68-baf7-44be-a8ee-9078cb6ac790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of h1n1_vaccine and of seasonal_vaccine -> could have been seen also by the mean\n",
    "\n",
    "print( f\"percentage h1n1_vaccine:   {y_train['h1n1_vaccine'].sum()/len(y_train)*100}\")\n",
    "print( f\"percentage seasonal_vaccine:   {y_train['seasonal_vaccine'].sum()/len(y_train)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30429fd4-807c-46a0-8677-2614f11219d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look to the distribution of missing values \n",
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b42a8f7-74a3-4f91-b87c-6799542bfaa9",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50cf1daf-a9c7-40ac-8f96-e45ddebcc1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to apply the preprocessing to both X_training and X_test, I create then a sklearn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "979155b1-914e-46f4-bfd4-61f19e45d607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems that nans in some categories as emlpoyment industry and employment occuapations may be seen as a category itself (\"unemployed\")\n",
    "# I would rather not delete those observations that contains nans and I ll try to input instead a sensible values - (is risky but normally people doesnt want to share bad information)\n",
    "#There are not too many columns so i will apply the change column by column, i would keep some of them as nan and try to input the values later\n",
    "def fill_missing_values_with_adhoc_values(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"employment_occupation\"] = df[\"employment_occupation\"].fillna(\"unemployed\")\n",
    "    df[\"employment_industry\"] = df[\"employment_industry\"].fillna(\"unemployed\")\n",
    "    df[\"education\"] = df[\"education\"].fillna(\"< 12 Years\")\n",
    "    df[\"health_insurance\"] = df[\"health_insurance\"].fillna(0)\n",
    "    df[\"income_poverty\"] = df[\"income_poverty\"].fillna(\"Below Poverty\")\n",
    "    df[\"employment_status\"] = df[\"employment_status\"].fillna(\"Unemployed\")\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed077a76-0dc4-4f1d-ae34-30bf69fb62b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I m gonna fill the other missing values on the one hot encoded nana columns with KNNImputer, but first I need to convert the columns to one hot encoded \n",
    "# if they dont have an order or scale them to integer \n",
    "\n",
    "def convert_columns_to_one_hot_encoded(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    categorical_columns = [\"race\", \"sex\",\"marital_status\",\"rent_or_own\",\"employment_status\",\"hhs_geo_region\",\"census_msa\", \"employment_industry\", \"employment_occupation\"]\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop=\"if_binary\")\n",
    "    one_hot_encoded = encoder.fit_transform(df[categorical_columns])\n",
    "    df = pd.concat([df, pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))], axis=1)\n",
    "    df = df.drop(categorical_columns, axis=1)\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b6be8d6-2d82-4641-90c6-c77b6270c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets convert the categorical variables ordered variables to numeric variables \n",
    "\n",
    "def map_categorical_ordered_variables(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    category_mapping = {'Below Poverty': 1, '<= $75,000, Above Poverty': 2, '> $75,000': 3}\n",
    "    df['income_poverty'] = df['income_poverty'].map(category_mapping)\n",
    "    \n",
    "    category_mapping = {'< 12 Years': 1, '12 Years': 2, 'Some College': 3, 'College Graduate': 4}\n",
    "    df['education'] = df['education'].map(category_mapping)\n",
    "    \n",
    "    category_mapping = {'18 - 34 Years': 1, '35 - 44 Years': 2, '45 - 54 Years': 3, '55 - 64 Years' : 4, '65+ Years' : 5 }\n",
    "    df['age_group'] = df['age_group'].map(category_mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f731efc-729f-4172-9f0e-92ed46714614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets imput missing values, i could use both the dataframe togheter for that but they should be big enough to make it robust\n",
    "def imputing_missing_values(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    knn_imputer = KNNImputer(n_neighbors=10)\n",
    "    imputed_data = knn_imputer.fit_transform(df)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "749fcf83-5e86-4ea7-9029-17615d903cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = Pipeline(\n",
    "    steps=[(\"fill_known_missing_values\", FunctionTransformer(fill_missing_values_with_adhoc_values)),\n",
    "           (\"convert_columns_to_one_hot_encoded\", FunctionTransformer(convert_columns_to_one_hot_encoded)),\n",
    "           (\"map_categorical_ordered_variables\", FunctionTransformer(map_categorical_ordered_variables)),\n",
    "           (\"impute_missing_values\", FunctionTransformer(imputing_missing_values)),\n",
    "          ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26091f6d-445e-4c47-a554-8d42ae3267ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test = preprocessing_pipeline.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75fc65c9-ffd3-4617-a18c-5e4fb1a3568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test is just for the submission, to avoid overfitting I split the training set to train and validation\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ffb93a-0bfd-474a-8db4-78926488055f",
   "metadata": {},
   "source": [
    "## Start training and get the first predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a0867a-389d-412d-8001-7410ce1c9bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f57a3d5-911c-4739-8a2a-186a91d94ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ba308-8d28-4a7c-b26c-8618ccfe39f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5613fe-a391-40b8-9372-bbba0922415a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
