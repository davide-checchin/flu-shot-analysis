{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "147189f4-f887-445e-b94f-e7325e374436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/p8vjfvvx3qx7blvj4038zc480000gp/T/ipykernel_9967/2130804501.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b146eda1-3f2f-428a-90dc-e83ef1a0c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../data/training_set_features.csv\")\n",
    "y_train = pd.read_csv(\"../data/training_set_labels.csv\")\n",
    "X_test = pd.read_csv(\"../data/test_set_features.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bfd519-652e-48e0-af6d-c3e2fad7de90",
   "metadata": {},
   "source": [
    "## First looks to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10be7c-dd9a-4d5e-926c-1a919643bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6fafdc-0599-46c2-9b41-9f3e7d28bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.head(3))\n",
    "print(X_train.describe())\n",
    "print(y_train.head(3))\n",
    "print(y_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12439c68-baf7-44be-a8ee-9078cb6ac790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of h1n1_vaccine and of seasonal_vaccine -> could have been seen also by the mean\n",
    "\n",
    "print( f\"percentage h1n1_vaccine:   {y_train['h1n1_vaccine'].sum()/len(y_train)*100}\")\n",
    "print( f\"percentage seasonal_vaccine:   {y_train['seasonal_vaccine'].sum()/len(y_train)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30429fd4-807c-46a0-8677-2614f11219d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look to the distribution of missing values \n",
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b42a8f7-74a3-4f91-b87c-6799542bfaa9",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50cf1daf-a9c7-40ac-8f96-e45ddebcc1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to apply the preprocessing to both X_training and X_test, I create then a sklearn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "979155b1-914e-46f4-bfd4-61f19e45d607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems that nans in some categories as emlpoyment industry and employment occuapations may be seen as a category itself (\"unemployed\")\n",
    "# I would rather not delete those observations that contains nans and I ll try to input instead a sensible values - (is risky but normally people doesnt want to share bad information)\n",
    "#There are not too many columns so i will apply the change column by column, i would keep some of them as nan and try to input the values later\n",
    "def fill_missing_values_with_adhoc_values(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"employment_occupation\"] = df[\"employment_occupation\"].fillna(\"unemployed\")\n",
    "    df[\"employment_industry\"] = df[\"employment_industry\"].fillna(\"unemployed\")\n",
    "    df[\"education\"] = df[\"education\"].fillna(\"< 12 Years\")\n",
    "    df[\"health_insurance\"] = df[\"health_insurance\"].fillna(0)\n",
    "    df[\"income_poverty\"] = df[\"income_poverty\"].fillna(\"Below Poverty\")\n",
    "    df[\"employment_status\"] = df[\"employment_status\"].fillna(\"Unemployed\")\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed077a76-0dc4-4f1d-ae34-30bf69fb62b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I m gonna fill the other missing values on the one hot encoded nana columns with KNNImputer, but first I need to convert the columns to one hot encoded \n",
    "# if they dont have an order or scale them to integer \n",
    "\n",
    "def convert_columns_to_one_hot_encoded(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    categorical_columns = [\"race\", \"sex\",\"marital_status\",\"rent_or_own\",\"employment_status\",\"hhs_geo_region\",\"census_msa\", \"employment_industry\", \"employment_occupation\"]\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop=\"if_binary\")\n",
    "    one_hot_encoded = encoder.fit_transform(df[categorical_columns])\n",
    "    df = pd.concat([df, pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))], axis=1)\n",
    "    df = df.drop(categorical_columns, axis=1)\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b6be8d6-2d82-4641-90c6-c77b6270c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets convert the categorical variables ordered variables to numeric variables \n",
    "\n",
    "def map_categorical_ordered_variables(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    category_mapping = {'Below Poverty': 1, '<= $75,000, Above Poverty': 2, '> $75,000': 3}\n",
    "    df['income_poverty'] = df['income_poverty'].map(category_mapping)\n",
    "    \n",
    "    category_mapping = {'< 12 Years': 1, '12 Years': 2, 'Some College': 3, 'College Graduate': 4}\n",
    "    df['education'] = df['education'].map(category_mapping)\n",
    "    \n",
    "    category_mapping = {'18 - 34 Years': 1, '35 - 44 Years': 2, '45 - 54 Years': 3, '55 - 64 Years' : 4, '65+ Years' : 5 }\n",
    "    df['age_group'] = df['age_group'].map(category_mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f731efc-729f-4172-9f0e-92ed46714614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets imput missing values, i could use both the dataframe togheter for that but they should be big enough to make it robust\n",
    "def imputing_missing_values(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    knn_imputer = KNNImputer(n_neighbors=10)\n",
    "    imputed_data = knn_imputer.fit_transform(df)\n",
    "    return imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "291e4909-c506-4682-b9c1-69559237e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_columns(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.drop(columns=[\"respondent_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "749fcf83-5e86-4ea7-9029-17615d903cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = Pipeline(\n",
    "    steps=[(\"fill_known_missing_values\", FunctionTransformer(fill_missing_values_with_adhoc_values)),\n",
    "           (\"convert_columns_to_one_hot_encoded\", FunctionTransformer(convert_columns_to_one_hot_encoded)),\n",
    "           (\"map_categorical_ordered_variables\", FunctionTransformer(map_categorical_ordered_variables)),\n",
    "           (\"drop_useless_columns\", FunctionTransformer(drop_useless_columns)),\n",
    "           (\"impute_missing_values\", FunctionTransformer(imputing_missing_values)),    \n",
    "          ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26091f6d-445e-4c47-a554-8d42ae3267ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test = preprocessing_pipeline.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75fc65c9-ffd3-4617-a18c-5e4fb1a3568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test is just for the submission, to avoid overfitting I split the training set to train and validation\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ffb93a-0bfd-474a-8db4-78926488055f",
   "metadata": {},
   "source": [
    "## Start training and get the first predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7d6f022b-c17a-4964-8b36-720f0778d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19a0867a-389d-412d-8001-7410ce1c9bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction with a logistic regression with imbalanced classes to have a baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1f57a3d5-911c-4739-8a2a-186a91d94ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davide.checchin/.asdf/installs/python/3.10.12/lib/python3.10/site-packages/sklearn/utils/validation.py:1229: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/davide.checchin/.asdf/installs/python/3.10.12/lib/python3.10/site-packages/sklearn/utils/validation.py:1229: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "logreg_model_h1n1 = LogisticRegression(max_iter = 1000)\n",
    "logreg_model_h1n1.fit(X_train, y_train[[\"h1n1_vaccine\"]])\n",
    "y_pred_h1h1 = logreg_model_h1n1.predict(X_validation)\n",
    "y_pred_h1h1_probs = logreg_model_h1n1.predict_proba(X_validation)[:, 1]\n",
    "\n",
    "\n",
    "logreg_model_seasonal = LogisticRegression(max_iter = 1000)\n",
    "logreg_model_seasonal.fit(X_train, y_train[[\"seasonal_vaccine\"]])\n",
    "y_pred_seasonal = logreg_model_seasonal.predict(X_validation)\n",
    "y_pred_seasonal_probs = logreg_model_seasonal.predict_proba(X_validation)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4d9ba308-8d28-4a7c-b26c-8618ccfe39f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline h1n1 score:   0.7190492398457\n",
      "baseline seasonal score:      0.7821854173696531\n"
     ]
    }
   ],
   "source": [
    "h1n1_score = roc_auc_score(y_validation[\"h1n1_vaccine\"] ,y_pred_h1h1)\n",
    "seasonal_score = roc_auc_score(y_validation[\"seasonal_vaccine\"] ,y_pred_seasonal)\n",
    "print(f\"baseline h1n1 score:   {h1n1_score}\")\n",
    "print(f\"baseline seasonal score:      {seasonal_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "845c84c9-da7b-424f-a085-d69e6fbcda7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline h1n1 score with probabilities:   0.8603786904671862\n",
      "baseline seasonal score with probabilities:      0.8565199247344104\n"
     ]
    }
   ],
   "source": [
    "h1n1_score_probs = roc_auc_score(y_validation[\"h1n1_vaccine\"] ,y_pred_h1h1_probs)\n",
    "seasonal_score_probs = roc_auc_score(y_validation[\"seasonal_vaccine\"] ,y_pred_seasonal_probs)\n",
    "print(f\"baseline h1n1 score with probabilities:   {h1n1_score_probs}\")\n",
    "print(f\"baseline seasonal score with probabilities:      {seasonal_score_probs}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "877c0f78-6f5a-4f47-bd0b-0834df5a7375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again with undersample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "acd6cee6-b4e3-47bf-a54a-4a4659e9a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_resampled_h1n1, y_resampled_h1n1 = under_sampler.fit_resample(X_train, y_train[\"h1n1_vaccine\"])\n",
    "X_resampled_seasonal, y_resampled_seasonal = under_sampler.fit_resample(X_train, y_train[\"seasonal_vaccine\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5990fe79-c35b-4fd9-9c95-221cba133283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using probabilities it s not stricktly necessary to resample datas \n",
    "\n",
    "logreg_model_h1n1 = LogisticRegression(max_iter = 1000)\n",
    "logreg_model_h1n1.fit(X_resampled_h1n1, y_resampled_h1n1)\n",
    "y_pred_h1h1 = logreg_model_h1n1.predict(X_validation)\n",
    "y_pred_h1h1_probs = logreg_model_h1n1.predict_proba(X_validation)[:, 1]\n",
    "\n",
    "logreg_model_seasonal = LogisticRegression(max_iter = 1000)\n",
    "logreg_model_seasonal.fit(X_resampled_seasonal, y_resampled_seasonal)\n",
    "y_pred_seasonal = logreg_model_seasonal.predict(X_validation)\n",
    "y_pred_seasonal_probs = logreg_model_seasonal.predict_proba(X_validation)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "26c5b4fc-9139-4297-b7ec-089a054fdbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline h1n1 score:   0.78393485952483\n",
      "baseline seasonal score:      0.7871126377235955\n"
     ]
    }
   ],
   "source": [
    "h1n1_score = roc_auc_score(y_validation[\"h1n1_vaccine\"] ,y_pred_h1h1)\n",
    "seasonal_score = roc_auc_score(y_validation[\"seasonal_vaccine\"] ,y_pred_seasonal)\n",
    "print(f\"baseline h1n1 score:   {h1n1_score}\")\n",
    "print(f\"baseline seasonal score:      {seasonal_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4b657cc8-c79d-4a95-8266-7d827ba6e6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline h1n1 score with probabilities:   0.8597204363428551\n",
      "baseline seasonal score with probabilities:      0.8563501495447046\n"
     ]
    }
   ],
   "source": [
    "h1n1_score = roc_auc_score(y_validation[\"h1n1_vaccine\"] ,y_pred_h1h1_probs)\n",
    "seasonal_score = roc_auc_score(y_validation[\"seasonal_vaccine\"] ,y_pred_seasonal_probs)\n",
    "print(f\"baseline h1n1 score with probabilities:   {h1n1_score}\")\n",
    "print(f\"baseline seasonal score with probabilities:      {seasonal_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14a288-ee8a-4464-abda-83471f013cde",
   "metadata": {},
   "source": [
    "## Proceed with more advanced models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aaef5a-9804-448a-aef5-fee381ae6d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ed54c-e9b3-41e9-9ea5-0a7788e59e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a2d962-29eb-4a69-b1b2-5287591f6358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e390ad32-10f0-4aa2-85ed-213e0cc28cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5238c7-de2b-4975-87bf-54cb8ca17f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
